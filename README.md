# Newcastle Racing 2025/2026

To start working on this project, clone the respository and ensure you are on the right branch.

```bash
git clone --recurse-submodules https://github.com/NewcastleRacingAI/Newcastle-Racing.git
cd Newcastle-Racing
git checkout 2526
git submodule update --init --recursive
```

## Project structure

This is the main workspace repository containing all the sub-packages that make up the Newcastle Racing AI project.

```bash
Newcastle-Racing
├── src # Ros packages
│   ├── zed-ros2-wrapper # Ready
│   ├── eufs_msgs # Ready
│   ├── ros_can # Ready
│   ├── nrai_odometry # TODO
│   ├── nrai_perception # TODO
│   ├── nrai_path_planning # 1  - TODO
│   ├── ft-fsd-path-planning # 1 
│   ├── nrai_controller # TODO
│   └── nrai_mission_control # TODO
└── hello_world.sh # Entry point script
```

## Running the full ROS workspace

First, install the ROS dependencies with `rosdep`

```bash
# Initialize the rosdep package manager (it may need sudo)
rosdep init || echo "rosdep already initialized"
# Update rosdep
rosdep update
# Install the ros dependencies
rosdep install --from-paths src --ignore-src -r -y
```

Then, build the ROS packages

```bash
/opt/ros/humble/setup.sh
colcon build --symlink-install
```


## Launching the ZED camera

To enable custom object detection for the zed camera, change the following:

```yaml
# common_stereo.yaml

# ...
        object_detection:
            od_enabled: true # True to enable Object Detection
            enable_tracking: true # Whether the object detection system includes object tracking capabilities across a sequence of images.
            detection_model: 'CUSTOM_YOLOLIKE_BOX_OBJECTS' # 'MULTI_CLASS_BOX_FAST', 'MULTI_CLASS_BOX_MEDIUM', 'MULTI_CLASS_BOX_ACCURATE', 'PERSON_HEAD_BOX_FAST', 'PERSON_HEAD_BOX_ACCURATE', 'CUSTOM_YOLOLIKE_BOX_OBJECTS'
            max_range: 20.0 # [m] Upper depth range for detections.The value cannot be greater than 'depth.max_depth'
            filtering_mode: 'NMS3D' # Filtering mode that should be applied to raw detections: 'NONE', 'NMS3D', 'NMS3D_PER_CLASS'
            prediction_timeout: 2.0 # During this time [sec], the object will have OK state even if it is not detected. Set this parameter to 0 to disable SDK predictions
            allow_reduced_precision_inference: false # Allow inference to run at a lower precision to improve runtime and memory usage
            # Other parameters are defined in the 'object_detection.yaml' and 'custom_object_detection.yaml' files
# ...
```

```yaml
# Custom 

# ...
/**:
  ros__parameters:
      object_detection:
          custom_onnx_file: '.../yolo11s.onnx' # Path to the YOLO-like ONNX file for custom object detection directly performed by the ZED SDK
          custom_onnx_input_size: 672 # Resolution used with the YOLO-like ONNX file. For example, 512 means a input tensor '1x3x512x512' 
          
          custom_class_count: 5 # Number of classes in the custom ONNX file. For example, 80 for YOLOv8 trained on COCO dataset

          # TODO: Add one instance of each class to the list below
          # Note: create a class_XXX identifier for each class in the custom ONNX file.
          # Note: XXX is a number from 000 to 'custom_class_count-1', and it must be unique for each class.
          # Note: the class_XXX identifier is not required to match the class ID [model_class_id] in the custom ONNX file.

          class_000:
            label: 'blue_cone'
            model_class_id: 0 # Class ID of the object in the custom ONNX file (it is not required that this value matches the value in the 'class_XXX' identifier)
            enabled: true # Enable/disable the detection of this class
            confidence_threshold: 50.0 # Minimum value of the detection confidence of an object [0,99]
            is_grounded: true # Provide hypothesis about the object movements (degrees of freedom or DoF) to improve the object tracking
            is_static: true # Provide hypothesis about the object staticity to improve the object tracking
            tracking_timeout: -1.0 # Maximum tracking time threshold (in seconds) before dropping the tracked object when unseen for this amount of time
            tracking_max_dist: -1.0 # Maximum tracking distance threshold (in meters) before dropping the tracked object when unseen for this amount of meters. Only valid for static object
            max_box_width_normalized: -1.0 # Maximum allowed width normalized to the image size
            min_box_width_normalized: -1.0 # Minimum allowed width normalized to the image size
            max_box_height_normalized: -1.0 # Maximum allowed height normalized to the image size
            min_box_height_normalized: -1.0 # Minimum allowed height normalized to the image size
            max_box_width_meters: -1.0 # Maximum allowed 3D width
            min_box_width_meters: -1.0 # Minimum allowed 3D width
            max_box_height_meters: -1.0 # Maximum allowed 3D height
            min_box_height_meters: -1.0 # Minimum allowed 3D height
            object_acceleration_preset: 'DEFAULT' # Object acceleration preset. Possible values: 'DEFAULT', 'LOW', 'MEDIUM', 'HIGH'
            max_allowed_acceleration: 100000.0 # If set with a different value from the default [100000], this value takes precedence over the selected preset, allowing for a custom maximum acceleration. Unit is m/s^2.
# ...
```

```bash
# Make sure both the global and local setup are sourced
source /opt/ros/humble/setup.bash && source install/setup.bash 
# Launch the zed camera
ros2 launch zed_wrapper zed_camera.launch.py camera_model:=zed2i custom_object_detection_config_path:=.../src/nrai_perception/resource/custom_object_detection.yaml
# Optionally, launch the rviz2 tool for a visualization of the camera topics
# Note that you will need to build the zed_display_rviz2 ROS package
ros2 launch zed_display_rviz2 display_zed_cam.launch.py camera_model:=zed2i
```
